---
title: "[Software-R] Unit 3: 3.2: Requirements Validation and Technical Review"

categories:
  - Software-R
tags:
  - [Validation, Technical Review]

toc: true
toc_sticky: true

date: 2023-06-01
last_modified_at: 2023-06-01
---

<!-- {% capture notice-2 %}

üìã This is the tech-news archives to help me keep track of what I am interested in!

- Reference tech news link: <https://thenextweb.com/news/blockchain-development-tech-career>
  {% endcapture %}

<div class="notice--danger">{{ notice-2 | markdownify }}</div> -->

üìã This is my note-taking from what I learned in the class "Software Requirements Engng"
{: .notice--danger}

<br>

# Requirements Validation and Technical Review

Recall from the previous unit one of the requirements engineering tasks was "`requirements validation`", a requirements validation checklist was discussed. In this topic we discuss the primary `requirements validation mechanism` which is Technical reviews. The review team that validates the requirements includes software engineers, customers, users, and other stakeholders who examine the `specification` looking for `errors` in content or interpretation, areas where clarification may be required, missing information, inconsistencies (a major problem when large products or systems are engineered), conflicting requirements, or unrealistic (un-achievable) requirements.

Looking at the figure below, notice the requirements engineering tasks in relation to the software engineering process and the main activities and umbrella activities.

![img](../../../assets/images/3.2.jpg)

For Example: consider two seemingly innocuous requirements:

1. The software should be user friendly.
   : The first requirement is too vague for developers to test or assess. What exactly does ‚Äúuser friendly‚Äù mean? To validate it, it must be `quantified or qualified` in some manner.
2. The probability of a successful unauthorized database intrusion should be less than 0.0001.
   : The second requirement has a quantitative element (‚Äúless than 0.0001‚Äù), but intrusion testing will be `difficult and time consuming`. Is this level of security even warranted for the application? Can other complementary requirements associated with security (e.g., password protection, specialized handshaking) replace the quantitative requirement noted?

<br>

# Technical review

The primary requirements validation happens through a `"Technical review" (TR)`

So, what is a Software review?

Software reviews are a `"Filter"` for the software process Workflow.

| Too few       | Too many              |
| :------------ | :-------------------- |
| flow is dirty | flow slows to trickle |

<br>

# Defect Amplification and Removal

1. Defect amplification
   : It is a term used to describe how an defect introduced early in the software engineering work flow (for example: during requirement modeling) and undetected, can and often will be amplified into multiple errors during design and more errors in construction.
2. Defect propagation
   : It is a term used to describe the impact an undiscovered defect has on future development activities or product behavior.
3. Technical debt
   : It is the term used to describe the costs incurred by failing to find and fix defects early or failing to update documentation following software changes.

The figure below shows that if proper technical reviews (inspections) are carried out then an earlier project deployment is possible,

![img](../../../assets/images/3.4.jpg)

<br>

# The review process

Reviews are applied at various points during the software engineering process and uncover `errors and defects & bugs`. There are many types of reviews that happens at different points of the software process. A technical review(TR) exemplified by casual reviews, walk-through and inspections

- Formal
- Informal
- Somewhere in between

The level of formality is chosen to match the the type of product to be built.

As shown in the figure below, The formality of a review increases when:

1. Distinct roles are explicitly defined for the reviewer.
2. There is a sufficient amount of planning and preparation for the review.
3. A distinct structure for the review (including tasks and internal work products) is defined.
4. Follow-up by the reviewers occurs for any corrections that are made.

![img](../../../assets/images/3.5.jpg)

1. Informal meetings
   : The benefit is immediate discovery of errors and better work product quality. Informal reviews include:
   : - A simple desk check of a software engineering work product with a colleague.
   : - A casual meeting (involving more than 2 people) for the purpose of reviewing a work product, or
   : - The review-oriented aspects of pair programming which encourages continuous review as work is created.
2. Formal meetings
   : The objectives of an Formal Technical Review (FTR), walk through or inspection, are:
   : - To uncover errors in function, logic, or implementation for any representation of the software.
   : - To verify that the software under review meets its requirements.
   : - To ensure that the software has been represented according to predefined standards.
   : - To achieve software that is developed in a uniform manner.
   : - To make projects more manageable.
3. Review Meeting
   : - Between three and five people (typically) should be involved in the review.
   : - Advance preparation should occur but should require no more than two hours of work for each person.
   : - The duration of the review meeting should be less than two hours.
   : - Focus is on a work product (for example: a portion of a requirements model, a detailed component design, source code for a component).
4. Review Players
   : - Producer ‚Äî the individual who has developed the work product.
   : - Review leader ‚Äî evaluates the product for readiness, generates copies of product materials, and distributes them to two or three reviewers for advance preparation and facilitates the meeting discussion.
   : - Reviewer(s) ‚Äî expected to spend between one and two hours reviewing the product, making notes, and otherwise becoming familiar with the work.
   : - Recorder ‚Äî reviewer who records (in writing) all important issues raised during the review.
5. Review Outcomes
   : At the end of the review, all attendees of the (FTR) must decide whether to:
   : - Accept the product without further modification.
   : - Reject the product due to severe errors (once corrected, another review must be performed).
   : - Accept the product provisionally (minor errors have been encountered and must be corrected, but no additional review will be required).
6. Review Reporting and Record Keeping
   : During the (FTR), the recorder records all issues raised and summarizes these in a review issues list to serve as an action list for the producer.
   : A formal technical review summary report is created that answers three questions:
   : - What was reviewed?
   : - Who reviewed it?
   : - What were the findings and conclusions?

`Design activities` introduce `50 to 65%` of all software `defects`.

`Review activities` have been shown to be `75%` effective in uncovering design flaws.

Technical reviews are the most effective mechanism to find mistakes early in the software process. The sooner you find a defect the cheaper it is to fix it. The output of a review is a list of issues and/or errors that have been uncovered.

<br>

# Review metrics

Software quality problems are referred to by various names `Errors, Defects, Bugs`. The goal is to detect as many as possible of these problems. Let us first define these terms:

- Defects & Bugs:
  : A quality problem found only `after` the software has been released to end users or other stakeholders
- Error:
  : A quality problem found only `before` the software has been released to end users or other stakeholders

We want to avoid defects because they make software engineers look bad.

The following review metrics can be collected for each review that is conducted:

1. Preparation effort, _Ep_ ‚Äî the effort (in person-hours) required to review a work product prior to the actual review meeting
2. Assessment effort, _Ea_ ‚Äî the effort (in person-hours) that is expended during the actual review
3. Rework effort, _Er_ ‚Äî the effort (in person-hours) that is dedicated to the correction of those errors uncovered during the review
4. Work product size, _WPS_ ‚Äî a measure of the size of the work product that has been reviewed (e.g., the number of UML models, or the number of document pages, or the number of lines of code)
5. Minor errors found, _Err<sub>minor</sub>_ ‚Äî the number of errors found that can be categorized as minor (requiring less than some pre-specified effort to correct)
6. Major errors found, _Err<sub>major</sub>_ ‚Äî the number of errors found that can be categorized as major (requiring more than some pre-specified effort to correct)
7. Total errors found, _Err<sub>tot</sub>_ - Represents the sum of the errors found: _Err<sub>tot</sub>_ = _Err<sub>minor</sub>_ + _Err<sub>major</sub>_
8. Error density Represents the error found per unit of work product reviewed:
9. Error density = _Err<sub>tot</sub>_ / _WPS_

These metrics can be further refined by associating the type of work product that was reviewed for the metrics collected.

How might these metrics be used for requirements engineering validation?

<br>

# Examples

## Example 1

If a requirements model is reviewed to uncover errors, inconsistencies, and omissions, it would be possible to compute the error density in a number of different ways.

Assume the requirements model contains 18 UML diagrams as part of 32 overall pages of descriptive materials.

The review uncovers:

18 minor errors and 4 major errors.

Therefore:

Err<sub>tot</sub> = 22

WPS is either number of UMLs or number of overall pages of descriptive material.

Therefore:

Error density is = 22/18 = 1.2 errors per UML diagram.

or

Error density is = 22/32 = 0.68 errors per requirements model page.

If reviews are conducted for a number of different types of work products (e.g., requirements model, design model, code, test cases), the percentage of errors uncovered for each review can be computed against the total number of errors found for all reviews. In addition, the error density for each work product can be computed.

Once data are collected for many reviews conducted across many projects,average values for error density enable you to estimate the number of errors to be found in a new (as yet un-reviewed document).

For example:

if the average error density for a requirements model is 0.6 errors per page, and a new requirement model is 32 pages long, a rough estimate suggests that your software team will find about 19 or 20 errors during the review of the document.

If you find only 6 errors, you‚Äôve done an extremely good job in developing the requirements model

Or

your review approach was not thorough enough.

Once testing has been conducted, it is possible to collect additional error data, including the effort required to find and correct errors uncovered during testing and the error density of the software.

The costs associated with finding and correcting an error during testing can be compared to those for reviews.

<br>

## Example 2

Building on Example, assume:
The effort required to correct a minor model error was found to require 4 person-hours.

The effort required for a major requirement error was found to be 18 person-hours.

Examining the review data collected, you find that minor errors occur about 6 times more frequently than major errors.
Accordingly: 1 major error (18 person hours) and 6 minor errors (6\*4 person hour=24) will mean a total of 42 person hours for 7 errors. The average per error is 42/7 = 6. Therefore, you can estimate that the average effort to find and correct a requirements error during review is about 6 person-hours.

Requirements related errors uncovered during testing require an average of 45 person-hours to find and correct. Using the averages noted, we get:

Effort saved per error = Etesting ‚Äì Ereviews

Since 22 errors were found during the review of the requirements model, a saving of about 22 \* 39 = 858 person-hours of testing effort would be achieved. And that‚Äôs just for requirements-related errors.

{% capture notice-2 %}

In Summary, the intent of every technical review is to find `errors` and `uncover issues` that would have a negative impact on the software to be deployed. The sooner an error is uncovered and corrected, the less likely that error will `propagate` to other software engineering work products and amplify itself, resulting in significantly more effort to correct it.
{% endcapture %}

<div class="notice--danger">{{ notice-2 | markdownify }}</div>

<br>

---

<br>

[Back to Top](#){: .btn .btn--primary }{: .align-right}
